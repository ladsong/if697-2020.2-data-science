{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ladsong/if697-2020.2-data-science/blob/projeto2/Projeto_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP7bSbbR0eNH"
      },
      "source": [
        "##Preparando o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZrgWPKwwqDA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "import warnings\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6U08MfwzZqI",
        "outputId": "aa55c7d1-793a-4a0c-fc31-b18166837ad1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmM_Ob5nzhzR",
        "outputId": "01b7d5d7-c8dc-4241-8f43-1d5915f2c7c7"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# /content/gdrive/My Drive/Kaggle é o caminho onde o arquivo kaggle.json está presente do Google Drive\n",
        "#Mudar o diretorio\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPNFhH6zqyn"
      },
      "source": [
        "##Carregando o dataset e processando os dados\n",
        "No projeto 1, foi adicionado o dump dos dados para que possamos utiliza-los aqui. O objetivo desse projeto é predizer a popularidade de uma música através de suas caracteristicas informadas no subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "950oJnwAzuQa"
      },
      "source": [
        "df = pd.read_csv('project1_output.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDXvjdgL1E3C"
      },
      "source": [
        "Observamos que o dataset é grande e assim decidimos utilizar um subset  com o objetivo de diminuir o tempo de experimentação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAzyCDNI1B2p",
        "outputId": "471b898a-c4ad-46f2-f990-a279f77e2eb2"
      },
      "source": [
        "df = df[:5000]\n",
        "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "df.dtypes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year                   int64\n",
              "acousticness         float64\n",
              "artists_data          object\n",
              "danceability         float64\n",
              "duration_ms            int64\n",
              "energy               float64\n",
              "explicit               int64\n",
              "id                    object\n",
              "instrumentalness     float64\n",
              "loudness             float64\n",
              "name                  object\n",
              "popularity           float64\n",
              "release_date          object\n",
              "speechiness          float64\n",
              "tempo                float64\n",
              "first_artist          object\n",
              "artists_by_artist     object\n",
              "first_genre           object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PG6pe_RQJk3"
      },
      "source": [
        "Tirando colunas desnecessárias, como `artist_data, id, name, release_date, first_artist, artists_by_artist` e `first_genre,` pois só queremos trabalhar com valores numéricos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmVGx9hJQW0B"
      },
      "source": [
        "df = df.select_dtypes(exclude=['object'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggg_wtCGQc-w",
        "outputId": "0685a31b-2d3f-425e-fc49-d09784ae1c74"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['year', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
              "       'explicit', 'instrumentalness', 'loudness', 'popularity', 'speechiness',\n",
              "       'tempo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48qY0V8RXrQ"
      },
      "source": [
        "\n",
        "\n",
        "###Escolher coluna para predição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30gxMAGvRe4u"
      },
      "source": [
        "target_col = df['popularity']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-zjSp-7SPB-",
        "outputId": "37d78405-3279-4bab-da38-2fde86b54bdb"
      },
      "source": [
        "target_col"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0.04\n",
              "1       0.02\n",
              "2       0.04\n",
              "3       0.00\n",
              "4       0.01\n",
              "        ... \n",
              "4995    0.00\n",
              "4996    0.00\n",
              "4997    0.00\n",
              "4998    0.00\n",
              "4999    0.00\n",
              "Name: popularity, Length: 5000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ6TKxGGSdUq"
      },
      "source": [
        "df = df.drop(columns=['popularity'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4z9GRaS2li"
      },
      "source": [
        "##Separando os dados em teste e predição\n",
        "Para realizacão do treinamento, teste e vaidacão, iremos separa o subset em: \n",
        "\n",
        "*   3/5 dos dados para treinamento\n",
        "*   1/5 dos dados para teste \n",
        "*   1/5 dos dados para validacão\n",
        "\n",
        "Seguindo assim, uma base 60/20/20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llnzPoPOS8-l"
      },
      "source": [
        "def get_x_data():\n",
        "    # input \n",
        "    train, val, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
        "    \n",
        "    return train, val, test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWlYxs_nvjhR"
      },
      "source": [
        "def get_y_data():\n",
        "    # output\n",
        "    train_labels, val_labels, test_labels = (\n",
        "        np.split(\n",
        "            target_col, \n",
        "            [int(.6*len(target_col)), int(.8*len(target_col))])\n",
        "    )\n",
        "    \n",
        "    return train_labels, val_labels, test_labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lu5e9X9vsBw"
      },
      "source": [
        "##Escolher os 4 algoritmos para predição"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbNEFCzEsBU"
      },
      "source": [
        "Os algoritmos de predicão que iremos usar são:\n",
        "\n",
        "*   Regressão Linear\n",
        "*   Multilayer perceptron\n",
        "*   Random forests\n",
        "*   Gradient boost com lightgbm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh6drbfgHj1I",
        "outputId": "8f202d4c-96bf-4e7c-b9b9-f9afe9c78cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install mlflow --quiet"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 14.4 MB 63 kB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 65.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 69.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25h  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0cVUQd7JL_H",
        "outputId": "eb7a3494-57b6-4093-e3cd-a882632556e4"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.9.1-py3-none-any.whl (302 kB)\n",
            "\u001b[K     |████████████████████████████████| 302 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.22)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (5.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (2.8.2)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.4)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 84.4 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 97.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=a3755124bc14d89ea8f30cf7432824a38c329709fbce8e8bc3b21a1d5a467fb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, cmd2, colorlog, cmaes, cliff, optuna\n",
            "Successfully installed cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 colorama-0.4.4 colorlog-5.0.1 optuna-2.9.1 pbr-5.6.0 pyperclip-1.8.2 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n-LB6i-HaTv"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import optuna\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import lightgbm\n",
        "from lightgbm import LGBMRegressor"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdO3ZS2Jmyg"
      },
      "source": [
        "###Função de avaliação das metricas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Qm1Z9QJqJI"
      },
      "source": [
        "def eval_metrics(actual, pred):\n",
        "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
        "    mae = mean_absolute_error(actual, pred)\n",
        "    r2 = r2_score(actual, pred)\n",
        "    return rmse, mae, r2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnE6KnfvJ1WN",
        "outputId": "ac82c8fe-e173-49ac-a69a-2fbfd42cb25d"
      },
      "source": [
        "mlflow.sklearn.autolog()\n",
        "mlflow.tensorflow.autolog()\n",
        "mlflow.lightgbm.autolog()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021/08/17 01:51:42 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of lightgbm. If you encounter errors during autologging, try upgrading / downgrading lightgbm to a supported version, or try upgrading MLflow.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcv8mZ28J8MX"
      },
      "source": [
        "###Regressão Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-NWvytLJ7uY"
      },
      "source": [
        "def linear_regression(trial):\n",
        "    train, test, val = get_x_data()\n",
        "    train_labels, val_labels, test_labels = get_y_data()\n",
        "    \n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    np.random.seed(40)\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Linear Regression\"):\n",
        "        reg = LinearRegression()\n",
        "        reg.fit(train, train_labels)\n",
        "\n",
        "        predictions = reg.predict(val)\n",
        "\n",
        "        (rmse, mae, r2) = eval_metrics(val_labels, predictions)\n",
        "\n",
        "        print(\"Modelo de regressão linear\")\n",
        "        print(\"  RMSE: %s\" % rmse)\n",
        "        print(\"  MAE: %s\" % mae)\n",
        "        print(\"  R2: %s\" % r2)\n",
        "\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"mae\", mae)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "        \n",
        "        \n",
        "        gc.collect()\n",
        "        \n",
        "        return rmse"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyu7BWpJKFkd",
        "outputId": "38df0e78-5bc9-4cf3-aada-7459c7c128db"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(linear_regression, n_trials=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:05,071]\u001b[0m A new study created in memory with name: no-name-614d2d2b-0b88-42aa-970c-8ff6e73a0d1c\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Modelo de regressão linear\n",
            "  RMSE: 0.13007133766127812\n",
            "  MAE: 0.08361231897096476\n",
            "  R2: -0.05711240737137979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:06,995]\u001b[0m Trial 0 finished with value: 0.13007133766127812 and parameters: {}. Best is trial 0 with value: 0.13007133766127812.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMU8ss6ULFv_"
      },
      "source": [
        "###Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1ihwwcLHoe"
      },
      "source": [
        "def mlp(trial):\n",
        "    train, test, val = get_x_data()\n",
        "    train_labels, val_labels, test_labels = get_y_data()\n",
        "    \n",
        "    params = {\n",
        "        \"hidden_units\": trial.suggest_int(\"hidden_units\", 3, 15),\n",
        "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
        "        \"epochs\": trial.suggest_int(\"epochs\", 10, 50)\n",
        "    }\n",
        "    \n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    np.random.seed(40)\n",
        "    \n",
        "    with mlflow.start_run(run_name=\"MLP\"):\n",
        "        normalizer = preprocessing.Normalization(axis=-1)\n",
        "        normalizer.adapt(np.array(train))\n",
        "        \n",
        "        mlp_model = tf.keras.Sequential([\n",
        "            normalizer,\n",
        "            layers.Dense(units=params[\"hidden_units\"]),\n",
        "            layers.Dense(units=params[\"hidden_units\"]),\n",
        "            layers.Dense(units=params[\"hidden_units\"]),\n",
        "            layers.Dense(units=1),\n",
        "        ])\n",
        "\n",
        "        mlp_model.summary()\n",
        "        \n",
        "        mlp_model.compile(\n",
        "            optimizer=tf.optimizers.Adam(learning_rate=params[\"lr\"]),\n",
        "            loss='mean_squared_error'\n",
        "        )\n",
        "\n",
        "        history = mlp_model.fit(\n",
        "            train, train_labels,\n",
        "            validation_data=(test, test_labels),\n",
        "            epochs=params[\"epochs\"]\n",
        "        )\n",
        "        \n",
        "        predictions = mlp_model.predict(val)\n",
        "\n",
        "        (rmse, mae, r2) = eval_metrics(val_labels, predictions)\n",
        "\n",
        "        print(\"MLP model\")\n",
        "        print(\"  RMSE: %s\" % rmse)\n",
        "        print(\"  MAE: %s\" % mae)\n",
        "        print(\"  R2: %s\" % r2)\n",
        "\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"mae\", mae)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "        mlflow.log_params(trial.params)\n",
        "        mlflow.set_tags(\n",
        "            {\n",
        "                \"estimator_name\":\"MultiLayerPerceptron\",\n",
        "                \"estimator_class\":\"Keras\"\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        gc.collect()\n",
        "        \n",
        "        return rmse"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMWfjUoYLPpm",
        "outputId": "47f24150-3de5-41c3-9434-a822ffa6ce82"
      },
      "source": [
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(mlp, n_trials=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:23,333]\u001b[0m A new study created in memory with name: no-name-55685bdd-d895-499b-bda0-c45dd0c9ce38\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 142\n",
            "Trainable params: 121\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/19\n",
            " 3/94 [..............................] - ETA: 2s - loss: 0.3789 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0103s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.4791 - val_loss: 0.5618\n",
            "Epoch 2/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.5232\n",
            "Epoch 3/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4879\n",
            "Epoch 4/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4540\n",
            "Epoch 5/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.4233\n",
            "Epoch 6/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.3960\n",
            "Epoch 7/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2928 - val_loss: 0.3685\n",
            "Epoch 8/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2703 - val_loss: 0.3446\n",
            "Epoch 9/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2499 - val_loss: 0.3227\n",
            "Epoch 10/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2315 - val_loss: 0.3019\n",
            "Epoch 11/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2145 - val_loss: 0.2833\n",
            "Epoch 12/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1990 - val_loss: 0.2654\n",
            "Epoch 13/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1849 - val_loss: 0.2498\n",
            "Epoch 14/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1721 - val_loss: 0.2346\n",
            "Epoch 15/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1604 - val_loss: 0.2212\n",
            "Epoch 16/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1497 - val_loss: 0.2082\n",
            "Epoch 17/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1401 - val_loss: 0.1963\n",
            "Epoch 18/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1312 - val_loss: 0.1856\n",
            "Epoch 19/19\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1231 - val_loss: 0.1758\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpvul6swtx/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.49359967767518986\n",
            "  MAE: 0.24369908249007538\n",
            "  R2: -14.223260949058306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:31,138]\u001b[0m Trial 0 finished with value: 0.49359967767518986 and parameters: {'hidden_units': 5, 'lr': 2.3814351954450575e-05, 'epochs': 19}. Best is trial 0 with value: 0.49359967767518986.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 33        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 82\n",
            "Trainable params: 61\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/14\n",
            " 3/94 [..............................] - ETA: 2s - loss: 0.0685 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0089s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.0779 - val_loss: 0.0922\n",
            "Epoch 2/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0764 - val_loss: 0.0906\n",
            "Epoch 3/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0749 - val_loss: 0.0891\n",
            "Epoch 4/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0735 - val_loss: 0.0876\n",
            "Epoch 5/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0721 - val_loss: 0.0861\n",
            "Epoch 6/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0707 - val_loss: 0.0846\n",
            "Epoch 7/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0694 - val_loss: 0.0832\n",
            "Epoch 8/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0681 - val_loss: 0.0819\n",
            "Epoch 9/14\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0806\n",
            "Epoch 10/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0793\n",
            "Epoch 11/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0646 - val_loss: 0.0781\n",
            "Epoch 12/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0635 - val_loss: 0.0769\n",
            "Epoch 13/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0624 - val_loss: 0.0758\n",
            "Epoch 14/14\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0614 - val_loss: 0.0746\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpjvdr2gkk/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.3198517923770368\n",
            "  MAE: 0.1784492097396776\n",
            "  R2: -5.392276239026126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:35,967]\u001b[0m Trial 1 finished with value: 0.3198517923770368 and parameters: {'hidden_units': 3, 'lr': 1.0002275909239583e-05, 'epochs': 14}. Best is trial 1 with value: 0.3198517923770368.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 15)                165       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 682\n",
            "Trainable params: 661\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            " 3/94 [..............................] - ETA: 2s - loss: 0.8302 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0090s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 1.3579 - val_loss: 1.8653\n",
            "Epoch 2/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 1.2869 - val_loss: 1.7779\n",
            "Epoch 3/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 1.2176 - val_loss: 1.6998\n",
            "Epoch 4/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.1523 - val_loss: 1.6218\n",
            "Epoch 5/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.0933 - val_loss: 1.5440\n",
            "Epoch 6/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.0394 - val_loss: 1.4744\n",
            "Epoch 7/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.9890 - val_loss: 1.4108\n",
            "Epoch 8/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.9420 - val_loss: 1.3504\n",
            "Epoch 9/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.8969 - val_loss: 1.2927\n",
            "Epoch 10/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.8549 - val_loss: 1.2380\n",
            "Epoch 11/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.8152 - val_loss: 1.1901\n",
            "Epoch 12/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.7787 - val_loss: 1.1349\n",
            "Epoch 13/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.7435 - val_loss: 1.0878\n",
            "Epoch 14/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.7101 - val_loss: 1.0424\n",
            "Epoch 15/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 0.9995\n",
            "Epoch 16/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.6479 - val_loss: 0.9611\n",
            "Epoch 17/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.6195 - val_loss: 0.9175\n",
            "Epoch 18/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5923 - val_loss: 0.8804\n",
            "Epoch 19/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5671 - val_loss: 0.8428\n",
            "Epoch 20/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5420 - val_loss: 0.8112\n",
            "Epoch 21/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.7785\n",
            "Epoch 22/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4966 - val_loss: 0.7434\n",
            "Epoch 23/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4744 - val_loss: 0.7107\n",
            "Epoch 24/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4539 - val_loss: 0.6805\n",
            "Epoch 25/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.6515\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp40n2pbov/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.9366544678712866\n",
            "  MAE: 0.4562534762661811\n",
            "  R2: -53.81719074174262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:42,595]\u001b[0m Trial 2 finished with value: 0.9366544678712866 and parameters: {'hidden_units': 15, 'lr': 1.1608366715619757e-05, 'epochs': 25}. Best is trial 1 with value: 0.3198517923770368.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12)                132       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 478\n",
            "Trainable params: 457\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/44\n",
            " 3/94 [..............................] - ETA: 2s - loss: 0.8169 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0103s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 1.0089 - val_loss: 1.6584\n",
            "Epoch 2/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.6352 - val_loss: 1.1996\n",
            "Epoch 3/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4528 - val_loss: 0.9520\n",
            "Epoch 4/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.8074\n",
            "Epoch 5/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2779 - val_loss: 0.6035\n",
            "Epoch 6/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.2247 - val_loss: 0.5050\n",
            "Epoch 7/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1831 - val_loss: 0.4057\n",
            "Epoch 8/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1533 - val_loss: 0.3166\n",
            "Epoch 9/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1210 - val_loss: 0.2572\n",
            "Epoch 10/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1007 - val_loss: 0.2053\n",
            "Epoch 11/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.1628\n",
            "Epoch 12/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0699 - val_loss: 0.1302\n",
            "Epoch 13/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0589 - val_loss: 0.1037\n",
            "Epoch 14/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0500 - val_loss: 0.0872\n",
            "Epoch 15/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.0635\n",
            "Epoch 16/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0518\n",
            "Epoch 17/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 0.0363\n",
            "Epoch 18/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0283\n",
            "Epoch 19/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0212\n",
            "Epoch 20/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0158\n",
            "Epoch 21/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 0.0120\n",
            "Epoch 22/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0099\n",
            "Epoch 23/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0080\n",
            "Epoch 24/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0062\n",
            "Epoch 25/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0172 - val_loss: 0.0053\n",
            "Epoch 26/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0045\n",
            "Epoch 27/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0035\n",
            "Epoch 28/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0034\n",
            "Epoch 29/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0032\n",
            "Epoch 30/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0033\n",
            "Epoch 31/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0025\n",
            "Epoch 32/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0029\n",
            "Epoch 33/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0028\n",
            "Epoch 34/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0025\n",
            "Epoch 35/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0027\n",
            "Epoch 36/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0028\n",
            "Epoch 37/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0028\n",
            "Epoch 38/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0026\n",
            "Epoch 39/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0028\n",
            "Epoch 40/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0026\n",
            "Epoch 41/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0029\n",
            "Epoch 42/44\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0031\n",
            "Epoch 43/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0029\n",
            "Epoch 44/44\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0034\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpmgxpo6en/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.12930161105549232\n",
            "  MAE: 0.08456205781826749\n",
            "  R2: -0.04463802262743988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:52:52,185]\u001b[0m Trial 3 finished with value: 0.12930161105549232 and parameters: {'hidden_units': 12, 'lr': 0.00017541205378697122, 'epochs': 44}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 44        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 110\n",
            "Trainable params: 89\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            " 3/94 [..............................] - ETA: 2s - loss: 1.0810 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0097s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 4ms/step - loss: 0.9626 - val_loss: 0.8014\n",
            "Epoch 2/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.7179 - val_loss: 0.5964\n",
            "Epoch 3/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 0.4491\n",
            "Epoch 4/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.4130 - val_loss: 0.3413\n",
            "Epoch 5/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.2634\n",
            "Epoch 6/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2475 - val_loss: 0.2054\n",
            "Epoch 7/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1947 - val_loss: 0.1618\n",
            "Epoch 8/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1550 - val_loss: 0.1299\n",
            "Epoch 9/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1250 - val_loss: 0.1059\n",
            "Epoch 10/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1025 - val_loss: 0.0876\n",
            "Epoch 11/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0738\n",
            "Epoch 12/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.0631\n",
            "Epoch 13/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.0548\n",
            "Epoch 14/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0543 - val_loss: 0.0482\n",
            "Epoch 15/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0481 - val_loss: 0.0426\n",
            "Epoch 16/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0433 - val_loss: 0.0384\n",
            "Epoch 17/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0395 - val_loss: 0.0342\n",
            "Epoch 18/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 0.0314\n",
            "Epoch 19/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0335 - val_loss: 0.0280\n",
            "Epoch 20/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0314 - val_loss: 0.0260\n",
            "Epoch 21/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0229\n",
            "Epoch 22/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.0213\n",
            "Epoch 23/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0264 - val_loss: 0.0193\n",
            "Epoch 24/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0251 - val_loss: 0.0179\n",
            "Epoch 25/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0160\n",
            "Epoch 26/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0148\n",
            "Epoch 27/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0135\n",
            "Epoch 28/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0123\n",
            "Epoch 29/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 0.0113\n",
            "Epoch 30/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0104\n",
            "Epoch 31/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0096\n",
            "Epoch 32/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0089\n",
            "Epoch 33/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0082\n",
            "Epoch 34/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0186 - val_loss: 0.0076\n",
            "Epoch 35/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0071\n",
            "Epoch 36/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0066\n",
            "Epoch 37/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0061\n",
            "Epoch 38/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0175 - val_loss: 0.0057\n",
            "Epoch 39/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0172 - val_loss: 0.0054\n",
            "Epoch 40/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0051\n",
            "Epoch 41/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0047\n",
            "Epoch 42/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0045\n",
            "Epoch 43/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0042\n",
            "Epoch 44/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0043\n",
            "Epoch 45/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0039\n",
            "Epoch 46/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0039\n",
            "Epoch 47/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0038\n",
            "Epoch 48/50\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0035\n",
            "Epoch 49/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0035\n",
            "Epoch 50/50\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0034\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp4jogc82a/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.13218836548014673\n",
            "  MAE: 0.08491365575641394\n",
            "  R2: -0.09180334892194164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:02,740]\u001b[0m Trial 4 finished with value: 0.13218836548014673 and parameters: {'hidden_units': 4, 'lr': 0.00010954134878264786, 'epochs': 50}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 15)                165       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 682\n",
            "Trainable params: 661\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/22\n",
            " 3/94 [..............................] - ETA: 2s - loss: 1.1924 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.5952 - val_loss: 0.2867\n",
            "Epoch 2/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1471 - val_loss: 0.1136\n",
            "Epoch 3/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0647 - val_loss: 0.0571\n",
            "Epoch 4/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0372 - val_loss: 0.0165\n",
            "Epoch 5/22\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0091\n",
            "Epoch 6/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0043\n",
            "Epoch 7/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0038\n",
            "Epoch 8/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0036\n",
            "Epoch 9/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0029\n",
            "Epoch 10/22\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0025\n",
            "Epoch 11/22\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0028\n",
            "Epoch 12/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0023\n",
            "Epoch 13/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0039\n",
            "Epoch 14/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0034\n",
            "Epoch 15/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0033\n",
            "Epoch 16/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0027\n",
            "Epoch 17/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0032\n",
            "Epoch 18/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0029\n",
            "Epoch 19/22\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0029\n",
            "Epoch 20/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0035\n",
            "Epoch 21/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0041\n",
            "Epoch 22/22\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0028\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpghvtisk7/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.13079430760862965\n",
            "  MAE: 0.0832570045995526\n",
            "  R2: -0.06889647130882004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:09,680]\u001b[0m Trial 5 finished with value: 0.13079430760862965 and parameters: {'hidden_units': 15, 'lr': 0.0004745734924690593, 'epochs': 22}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 142\n",
            "Trainable params: 121\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            " 3/94 [..............................] - ETA: 2s - loss: 1.3909 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.6716 - val_loss: 0.3203\n",
            "Epoch 2/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.2183 - val_loss: 0.1171\n",
            "Epoch 3/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1008 - val_loss: 0.0593\n",
            "Epoch 4/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0332\n",
            "Epoch 5/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0204\n",
            "Epoch 6/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0123\n",
            "Epoch 7/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0078\n",
            "Epoch 8/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0060\n",
            "Epoch 9/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.0045\n",
            "Epoch 10/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 0.0046\n",
            "Epoch 11/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0034\n",
            "Epoch 12/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0036\n",
            "Epoch 13/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0028\n",
            "Epoch 14/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0024\n",
            "Epoch 15/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0034\n",
            "Epoch 16/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0036\n",
            "Epoch 17/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0032\n",
            "Epoch 18/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0034\n",
            "Epoch 19/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0037\n",
            "Epoch 20/25\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0039\n",
            "Epoch 21/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0029\n",
            "Epoch 22/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0035\n",
            "Epoch 23/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0043\n",
            "Epoch 24/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0018\n",
            "Epoch 25/25\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0034\n",
            "INFO:tensorflow:Assets written to: /tmp/tmplth4wcho/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.1294019073079829\n",
            "  MAE: 0.08443141276896\n",
            "  R2: -0.04625925399680919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:16,181]\u001b[0m Trial 6 finished with value: 0.1294019073079829 and parameters: {'hidden_units': 5, 'lr': 0.0006240213857318179, 'epochs': 25}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 142\n",
            "Trainable params: 121\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/29\n",
            " 3/94 [..............................] - ETA: 3s - loss: 0.7830 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0117s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.7503 - val_loss: 0.7476\n",
            "Epoch 2/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.6106 - val_loss: 0.6051\n",
            "Epoch 3/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4970 - val_loss: 0.4919\n",
            "Epoch 4/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4007\n",
            "Epoch 5/29\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3344 - val_loss: 0.3266\n",
            "Epoch 6/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2771 - val_loss: 0.2706\n",
            "Epoch 7/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2322 - val_loss: 0.2236\n",
            "Epoch 8/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1967 - val_loss: 0.1885\n",
            "Epoch 9/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1691 - val_loss: 0.1608\n",
            "Epoch 10/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1475 - val_loss: 0.1390\n",
            "Epoch 11/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1305 - val_loss: 0.1219\n",
            "Epoch 12/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1170 - val_loss: 0.1080\n",
            "Epoch 13/29\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1060 - val_loss: 0.0967\n",
            "Epoch 14/29\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.0873\n",
            "Epoch 15/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.0794\n",
            "Epoch 16/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0728\n",
            "Epoch 17/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0770 - val_loss: 0.0667\n",
            "Epoch 18/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0719 - val_loss: 0.0613\n",
            "Epoch 19/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0673 - val_loss: 0.0566\n",
            "Epoch 20/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0632 - val_loss: 0.0523\n",
            "Epoch 21/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0595 - val_loss: 0.0483\n",
            "Epoch 22/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0448\n",
            "Epoch 23/29\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0415\n",
            "Epoch 24/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0502 - val_loss: 0.0386\n",
            "Epoch 25/29\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0358\n",
            "Epoch 26/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0451 - val_loss: 0.0333\n",
            "Epoch 27/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0429 - val_loss: 0.0310\n",
            "Epoch 28/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0409 - val_loss: 0.0289\n",
            "Epoch 29/29\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0390 - val_loss: 0.0268\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpzzd98elb/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.19941255826246648\n",
            "  MAE: 0.1504811196243577\n",
            "  R2: -1.4846371085866075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:23,536]\u001b[0m Trial 7 finished with value: 0.19941255826246648 and parameters: {'hidden_units': 5, 'lr': 5.9367778049817834e-05, 'epochs': 29}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 13)                143       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 14        \n",
            "=================================================================\n",
            "Total params: 542\n",
            "Trainable params: 521\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/18\n",
            " 3/94 [..............................] - ETA: 3s - loss: 0.9640 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0126s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 0.2988 - val_loss: 0.0776\n",
            "Epoch 2/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0537 - val_loss: 0.0187\n",
            "Epoch 3/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0067\n",
            "Epoch 4/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0175 - val_loss: 0.0041\n",
            "Epoch 5/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0035\n",
            "Epoch 6/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0032\n",
            "Epoch 7/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0025\n",
            "Epoch 8/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0030\n",
            "Epoch 9/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0032\n",
            "Epoch 10/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0035\n",
            "Epoch 11/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0031\n",
            "Epoch 12/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0030\n",
            "Epoch 13/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0031\n",
            "Epoch 14/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0022\n",
            "Epoch 15/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0025\n",
            "Epoch 16/18\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0037\n",
            "Epoch 17/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0034\n",
            "Epoch 18/18\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0033\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpk3y2r98r/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.12954275149138028\n",
            "  MAE: 0.08390491405051202\n",
            "  R2: -0.04853804150723873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:29,450]\u001b[0m Trial 8 finished with value: 0.12954275149138028 and parameters: {'hidden_units': 13, 'lr': 0.0002725172979947129, 'epochs': 18}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization (Normalization (None, 10)                21        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 362\n",
            "Trainable params: 341\n",
            "Non-trainable params: 21\n",
            "_________________________________________________________________\n",
            "Epoch 1/37\n",
            " 3/94 [..............................] - ETA: 3s - loss: 1.5201 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 1.3910 - val_loss: 1.2909\n",
            "Epoch 2/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 1.1372 - val_loss: 1.0515\n",
            "Epoch 3/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.9317 - val_loss: 0.8516\n",
            "Epoch 4/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.7626 - val_loss: 0.6874\n",
            "Epoch 5/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.6233 - val_loss: 0.5535\n",
            "Epoch 6/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.4435\n",
            "Epoch 7/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3529\n",
            "Epoch 8/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.2802\n",
            "Epoch 9/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2739 - val_loss: 0.2210\n",
            "Epoch 10/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.2218 - val_loss: 0.1732\n",
            "Epoch 11/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1793 - val_loss: 0.1351\n",
            "Epoch 12/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.1449 - val_loss: 0.1058\n",
            "Epoch 13/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.0826\n",
            "Epoch 14/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0957 - val_loss: 0.0641\n",
            "Epoch 15/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0786 - val_loss: 0.0507\n",
            "Epoch 16/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.0404\n",
            "Epoch 17/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0550 - val_loss: 0.0327\n",
            "Epoch 18/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0471 - val_loss: 0.0270\n",
            "Epoch 19/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0410 - val_loss: 0.0226\n",
            "Epoch 20/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 0.0192\n",
            "Epoch 21/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0166\n",
            "Epoch 22/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0147\n",
            "Epoch 23/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0131\n",
            "Epoch 24/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0260 - val_loss: 0.0116\n",
            "Epoch 25/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.0106\n",
            "Epoch 26/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0097\n",
            "Epoch 27/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0089\n",
            "Epoch 28/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0082\n",
            "Epoch 29/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0076\n",
            "Epoch 30/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 0.0071\n",
            "Epoch 31/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0066\n",
            "Epoch 32/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0062\n",
            "Epoch 33/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0058\n",
            "Epoch 34/37\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0054\n",
            "Epoch 35/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0052\n",
            "Epoch 36/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0049\n",
            "Epoch 37/37\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0046\n",
            "INFO:tensorflow:Assets written to: /tmp/tmppb0h_ksw/model/data/model/assets\n",
            "MLP model\n",
            "  RMSE: 0.1368383551760044\n",
            "  MAE: 0.09100871795945802\n",
            "  R2: -0.16996709501653062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:41,708]\u001b[0m Trial 9 finished with value: 0.1368383551760044 and parameters: {'hidden_units': 10, 'lr': 5.2491145582008663e-05, 'epochs': 37}. Best is trial 3 with value: 0.12930161105549232.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM_kbNyrLssM"
      },
      "source": [
        "###Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7NqTLYULt3i"
      },
      "source": [
        "def random_forest(trial):\n",
        "    train, test, val = get_x_data()\n",
        "    train_labels, val_labels, test_labels = get_y_data()\n",
        "    \n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5),\n",
        "    }\n",
        "    \n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    np.random.seed(40)\n",
        "    \n",
        "    with mlflow.start_run(run_name=\"Random Forest\"):\n",
        "        rf = RandomForestRegressor(\n",
        "            max_depth=params[\"max_depth\"],\n",
        "            n_estimators=params[\"n_estimators\"],\n",
        "            min_samples_split=params[\"min_samples_split\"],\n",
        "            random_state=0\n",
        "        )\n",
        "        rf.fit(train, train_labels)\n",
        "        \n",
        "        predictions = rf.predict(val)\n",
        "        \n",
        "        (rmse, mae, r2) = eval_metrics(val_labels, predictions)\n",
        "        \n",
        "        print(\"Random Forest model\")\n",
        "        print(\"  RMSE: %s\" % rmse)\n",
        "        print(\"  MAE: %s\" % mae)\n",
        "        print(\"  R2: %s\" % r2)\n",
        "        \n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"mae\", mae)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "        mlflow.log_params(trial.params)\n",
        "        \n",
        "        gc.collect()\n",
        "        \n",
        "        return rmse"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdHSTFZeLxrs",
        "outputId": "9b15c8b3-2f6f-4c78-eba5-cbf379f169b0"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(random_forest, n_trials=10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:41,766]\u001b[0m A new study created in memory with name: no-name-d93f78eb-af08-48d5-8188-642013a8feac\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13084474164800078\n",
            "  MAE: 0.08465437294847354\n",
            "  R2: -0.06972095918238397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:44,180]\u001b[0m Trial 0 finished with value: 0.13084474164800078 and parameters: {'n_estimators': 137, 'max_depth': 7, 'min_samples_split': 5}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13100121422779862\n",
            "  MAE: 0.08449373537016633\n",
            "  R2: -0.0722809715929904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:45,540]\u001b[0m Trial 1 finished with value: 0.13100121422779862 and parameters: {'n_estimators': 106, 'max_depth': 3, 'min_samples_split': 3}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13105588553258968\n",
            "  MAE: 0.08456922308746868\n",
            "  R2: -0.07317615768512842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:47,757]\u001b[0m Trial 2 finished with value: 0.13105588553258968 and parameters: {'n_estimators': 136, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13153718351118723\n",
            "  MAE: 0.08545849212195081\n",
            "  R2: -0.08107303250575315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:50,008]\u001b[0m Trial 3 finished with value: 0.13153718351118723 and parameters: {'n_estimators': 104, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.1308736264888639\n",
            "  MAE: 0.0844663910850699\n",
            "  R2: -0.0701933072467067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:51,324]\u001b[0m Trial 4 finished with value: 0.1308736264888639 and parameters: {'n_estimators': 70, 'max_depth': 4, 'min_samples_split': 2}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.1309267376266287\n",
            "  MAE: 0.08467610507875187\n",
            "  R2: -0.07106209523617157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:52,632]\u001b[0m Trial 5 finished with value: 0.1309267376266287 and parameters: {'n_estimators': 59, 'max_depth': 6, 'min_samples_split': 5}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.1309932291727004\n",
            "  MAE: 0.08456717338299902\n",
            "  R2: -0.07215025583224977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:53,819]\u001b[0m Trial 6 finished with value: 0.1309932291727004 and parameters: {'n_estimators': 80, 'max_depth': 3, 'min_samples_split': 3}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13139459834586267\n",
            "  MAE: 0.08497714299845266\n",
            "  R2: -0.0787305552373998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:56,755]\u001b[0m Trial 7 finished with value: 0.13139459834586267 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 3}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13126225526177487\n",
            "  MAE: 0.08519574618498782\n",
            "  R2: -0.07655861462737845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:53:59,494]\u001b[0m Trial 8 finished with value: 0.13126225526177487 and parameters: {'n_estimators': 126, 'max_depth': 10, 'min_samples_split': 4}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest model\n",
            "  RMSE: 0.13102418006574745\n",
            "  MAE: 0.08485249464800479\n",
            "  R2: -0.07265696794925369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:01,066]\u001b[0m Trial 9 finished with value: 0.13102418006574745 and parameters: {'n_estimators': 67, 'max_depth': 8, 'min_samples_split': 5}. Best is trial 0 with value: 0.13084474164800078.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q5kxu8PMNpw"
      },
      "source": [
        "###Gradient boost com lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oegzHTIqMV-n"
      },
      "source": [
        "def gradient_boosting(trial):\n",
        "    train, test, val = get_x_data()\n",
        "    train_labels, val_labels, test_labels = get_y_data()\n",
        "    \n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 25, 35),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    }\n",
        "    \n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    np.random.seed(40)\n",
        "    \n",
        "    with mlflow.start_run(run_name=\"Gradient Boosting\"):\n",
        "        model = XGBRegressor(\n",
        "            max_depth=params[\"max_depth\"],\n",
        "            n_estimators=params[\"n_estimators\"],\n",
        "        )\n",
        "        model.fit(train, train_labels)\n",
        "        \n",
        "        predictions = model.predict(test)\n",
        "        print('Prediction: %.3f' % predictions[0])\n",
        "        \n",
        "        (rmse, mae, r2) = eval_metrics(val_labels, predictions)\n",
        "\n",
        "        print(\"LGBM model\")\n",
        "        print(\"  RMSE: %s\" % rmse)\n",
        "        print(\"  MAE: %s\" % mae)\n",
        "        print(\"  R2: %s\" % r2)\n",
        "\n",
        "        # Log mlflow attributes for mlflow UI\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"mae\", mae)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "        mlflow.log_params(trial.params)\n",
        "        mlflow.set_tags(\n",
        "            {\n",
        "                \"estimator_class\":\"LightGBM\",\n",
        "                \"estimator_name\":\"Gradient Boosting\"\n",
        "            }\n",
        "        )\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "        \n",
        "        gc.collect()\n",
        "        \n",
        "        return rmse"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MA5E8HHMbRd",
        "outputId": "10cb9eb2-c44d-46f7-e9f6-df482ea81863"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(gradient_boosting, n_trials=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:01,107]\u001b[0m A new study created in memory with name: no-name-3378c745-db54-4415-949f-c658fb28c8c2\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.049\n",
            "LGBM model\n",
            "  RMSE: 0.1345920713821432\n",
            "  MAE: 0.08818051055431367\n",
            "  R2: -0.13187094308558778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:02,025]\u001b[0m Trial 0 finished with value: 0.1345920713821432 and parameters: {'n_estimators': 143, 'num_leaves': 27, 'max_depth': 5}. Best is trial 0 with value: 0.1345920713821432.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.045\n",
            "LGBM model\n",
            "  RMSE: 0.13220877903262113\n",
            "  MAE: 0.08500739752054215\n",
            "  R2: -0.09214058443862028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:02,731]\u001b[0m Trial 1 finished with value: 0.13220877903262113 and parameters: {'n_estimators': 94, 'num_leaves': 33, 'max_depth': 4}. Best is trial 1 with value: 0.13220877903262113.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.059\n",
            "LGBM model\n",
            "  RMSE: 0.1318033902047479\n",
            "  MAE: 0.08500476626873014\n",
            "  R2: -0.08545324021982159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:03,759]\u001b[0m Trial 2 finished with value: 0.1318033902047479 and parameters: {'n_estimators': 86, 'num_leaves': 25, 'max_depth': 3}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.032\n",
            "LGBM model\n",
            "  RMSE: 0.1374720238643697\n",
            "  MAE: 0.09069421284317972\n",
            "  R2: -0.18082791073983007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:05,113]\u001b[0m Trial 3 finished with value: 0.1374720238643697 and parameters: {'n_estimators': 115, 'num_leaves': 26, 'max_depth': 10}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.031\n",
            "LGBM model\n",
            "  RMSE: 0.13418661898649747\n",
            "  MAE: 0.08770236937522889\n",
            "  R2: -0.1250617968497567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:05,910]\u001b[0m Trial 4 finished with value: 0.13418661898649747 and parameters: {'n_estimators': 55, 'num_leaves': 28, 'max_depth': 10}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.037\n",
            "LGBM model\n",
            "  RMSE: 0.1366341017621044\n",
            "  MAE: 0.08975215070009232\n",
            "  R2: -0.16647697088503377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:07,059]\u001b[0m Trial 5 finished with value: 0.1366341017621044 and parameters: {'n_estimators': 90, 'num_leaves': 33, 'max_depth': 10}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.027\n",
            "LGBM model\n",
            "  RMSE: 0.13558707743753967\n",
            "  MAE: 0.08761397820711136\n",
            "  R2: -0.1486680895825303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:07,930]\u001b[0m Trial 6 finished with value: 0.13558707743753967 and parameters: {'n_estimators': 91, 'num_leaves': 27, 'max_depth': 7}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.029\n",
            "LGBM model\n",
            "  RMSE: 0.13673111845888022\n",
            "  MAE: 0.08953086614370347\n",
            "  R2: -0.16813406700312372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:09,034]\u001b[0m Trial 7 finished with value: 0.13673111845888022 and parameters: {'n_estimators': 100, 'num_leaves': 28, 'max_depth': 8}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.047\n",
            "LGBM model\n",
            "  RMSE: 0.13184100254033523\n",
            "  MAE: 0.08483837148308754\n",
            "  R2: -0.08607283364196094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:09,750]\u001b[0m Trial 8 finished with value: 0.13184100254033523 and parameters: {'n_estimators': 74, 'num_leaves': 29, 'max_depth': 4}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[01:54:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 0.054\n",
            "LGBM model\n",
            "  RMSE: 0.1336722088954913\n",
            "  MAE: 0.08643259169697762\n",
            "  R2: -0.1164523866494318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-17 01:54:10,652]\u001b[0m Trial 9 finished with value: 0.1336722088954913 and parameters: {'n_estimators': 147, 'num_leaves': 30, 'max_depth': 3}. Best is trial 2 with value: 0.1318033902047479.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q4uGa-Qm343"
      },
      "source": [
        "# Análise de melhor modelo de predicão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXJXQXwK0ScD",
        "outputId": "7d289bdd-447d-451d-d2ff-98b364718cf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"MLflow on Colab\"):\n",
        "  mlflow.log_metric(\"m1\", 2.0)\n",
        "  mlflow.log_param(\"p1\", \"mlflow-colab\")\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n",
        "\n",
        "\n",
        "# create remote tunnel using ngrok.com to allow local port access\n",
        "# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://3633a99cf690.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}